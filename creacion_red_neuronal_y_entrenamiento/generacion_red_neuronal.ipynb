{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación del Código en Python\n",
    "\n",
    "Este bloque de código obtiene la ruta actual del script en ejecución y la modifica para apuntar a un directorio específico. A continuación, se detalla su funcionamiento.\n",
    "\n",
    "### Código\n",
    "```python\n",
    "try:\n",
    "    carpeta_actual = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    carpeta_actual = os.getcwd()  # Alternativa para entornos interactivos como Jupyter\n",
    "\n",
    "Carpeta_de_entrenamiento = carpeta_actual.replace('creacion_red_neuronal_y_entrenamiento', 'datos_iniciales_del_sistema')\n",
    "print(f\"Carpeta actual: {Carpeta_de_entrenamiento}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta actual: c:\\Users\\dgome\\OneDrive\\SIN RECUPERAR\\Sql\\Documentos\\SistemaIot\\Periferico\\modelos_entrenamiento\\vehiculo_autonomo\\datos_iniciales_del_sistema\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    carpeta_actual = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    carpeta_actual = os.getcwd()  # Alternativa para entornos interactivos como Jupyter\n",
    "\n",
    "Carpeta_de_entrenamiento=carpeta_actual.replace('creacion_red_neuronal_y_entrenamiento', 'datos_iniciales_del_sistema')\n",
    "print(f\"Carpeta actual: {Carpeta_de_entrenamiento}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación del Código en Python\n",
    "\n",
    "Este bloque de código construye una ruta para un archivo CSV en función de un directorio base, verifica si el archivo existe y luego intenta cargarlo usando la biblioteca `pandas`. Si ocurre algún error, el programa lo notifica y detiene su ejecución.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV cargado correctamente. Filas: 3737, Columnas: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ruta_csv = os.path.join(Carpeta_de_entrenamiento,'tinyml_training_data_clean.csv')\n",
    "# 3. Cargar el archivo CSV\n",
    "if os.path.exists(ruta_csv):\n",
    "    try:\n",
    "        data = pd.read_csv(ruta_csv)\n",
    "        print(f\"Archivo CSV cargado correctamente. Filas: {data.shape[0]}, Columnas: {data.shape[1]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo CSV: {e}\")\n",
    "        exit()\n",
    "else:\n",
    "    print(f\"El archivo CSV no existe: {ruta_csv}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación del Código en Python\n",
    "\n",
    "Este bloque de código carga un archivo CSV y verifica que las columnas necesarias para realizar operaciones posteriores estén presentes. Si alguna columna está ausente, se notifica al usuario y se detiene la ejecución del programa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(ruta_csv)\n",
    "# Verificar la existencia de las columnas necesarias\n",
    "columnas_entrada = ['distancia_izquierda', 'distancia_derecha', 'distancia_izquierda_duration', 'distancia_derecha_duration']\n",
    "columnas_salida = ['IN1', 'IN2', 'IN3', 'IN4']\n",
    "\n",
    "for columna in columnas_entrada + columnas_salida:\n",
    "    if columna not in data.columns:\n",
    "        print(f\"La columna '{columna}' no está presente en el archivo CSV.\")\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación \n",
    "\n",
    "Este bloque de código toma los datos del archivo CSV, selecciona las columnas necesarias para el análisis, verifica si hay problemas en los datos (como valores faltantes), y prepara los datos de entrada para que sean consistentes y fáciles de trabajar en el modelo.\n",
    "\n",
    "\n",
    "#### 1. Definir las Entradas y Salidas\n",
    "```python\n",
    "X = data[columnas_entrada].values\n",
    "y = data[columnas_salida].values\n",
    "```\n",
    "- **¿Qué significa esto?**\n",
    "  - Se separan los datos en dos grupos:\n",
    "    - **Entradas (`X`)**: La información que usamos para hacer predicciones o análisis (por ejemplo, mediciones o características de un sistema).\n",
    "    - **Salidas (`y`)**: Los resultados esperados o etiquetas que queremos predecir (por ejemplo, activaciones de un sistema o valores esperados).\n",
    "  - Los datos se seleccionan de las columnas específicas del archivo CSV, indicadas en las listas `columnas_entrada` y `columnas_salida`.\n",
    "\n",
    "#### 2. Validar si Hay Datos Faltantes\n",
    "```python\n",
    "if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "    print(\"Existen valores NaN en los datos de entrada o salida. Por favor, limpie los datos.\")\n",
    "    exit()\n",
    "```\n",
    "- **¿Qué hace esto?**\n",
    "  - Revisa si hay valores faltantes (llamados \"NaN\", que significa \"Not a Number\") en las entradas o salidas.\n",
    "  - Si encuentra datos faltantes:\n",
    "    - Muestra un mensaje indicando el problema: \"Existen valores NaN en los datos de entrada o salida. Por favor, limpie los datos.\"\n",
    "    - Detiene la ejecución del programa.\n",
    "- **¿Por qué es importante?**\n",
    "  - Los valores faltantes pueden causar errores graves durante el análisis o entrenamiento del modelo, por lo que deben ser eliminados o corregidos antes de continuar.\n",
    "\n",
    "#### 3. Normalizar las Entradas\n",
    "```python\n",
    "X_max = np.max(X, axis=0)\n",
    "if np.any(X_max == 0):\n",
    "    print(\"Error: Algunos valores máximos de las columnas de entrada son 0, lo que causará división por cero.\")\n",
    "    exit()\n",
    "X = X / X_max\n",
    "```\n",
    "- **¿Qué significa \"normalizar\"?**\n",
    "  - Es un proceso para convertir los datos en un rango más uniforme (en este caso, de 0 a 1).\n",
    "  - Esto se logra dividiendo cada valor de entrada por el valor máximo de su columna.\n",
    "- **¿Qué hace el código?**\n",
    "  - Encuentra el valor más alto (`X_max`) en cada columna de las entradas.\n",
    "  - Verifica si alguno de estos valores máximos es 0:\n",
    "    - Si es así, muestra un error: \"Algunos valores máximos de las columnas de entrada son 0, lo que causará división por cero.\"\n",
    "    - Detiene la ejecución.\n",
    "  - Si todo está bien, divide cada valor por su máximo correspondiente, haciendo que todos los datos estén en el rango 0-1.\n",
    "- **¿Por qué es importante?**\n",
    "  - Esto asegura que los datos sean consistentes y evita problemas matemáticos (como división por cero) que podrían ocurrir durante el análisis o entrenamiento del modelo.\n",
    "\n",
    "### Resumen General\n",
    "Este código:\n",
    "1. **Separa** los datos en entradas (`X`) y salidas (`y`) para un modelo.\n",
    "2. **Verifica** que no haya valores faltantes en los datos, ya que estos podrían causar problemas.\n",
    "3. **Prepara** los datos de entrada normalizándolos, asegurando que estén en un rango uniforme y evitando errores matemáticos.\n",
    "\n",
    "### Ejemplo para Entender Mejor\n",
    "- Si tus datos de entrada son:\n",
    "  ```\n",
    "  distancia_izquierda: [10, 20, 30]\n",
    "  distancia_derecha: [5, 10, 15]\n",
    "  ```\n",
    "  Después de normalizar, se convierten en:\n",
    "  ```\n",
    "  distancia_izquierda (normalizada): [0.33, 0.67, 1]\n",
    "  distancia_derecha (normalizada): [0.33, 0.67, 1]\n",
    "  ```\n",
    "  Esto ayuda a que el modelo trabaje con datos consistentes sin dar más peso a valores más grandes de manera injusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las entradas (X) y las salidas (y)\n",
    "X = data[columnas_entrada].values\n",
    "y = data[columnas_salida].values\n",
    "\n",
    "# Validar si hay datos faltantes\n",
    "if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "    print(\"Existen valores NaN en los datos de entrada o salida. Por favor, limpie los datos.\")\n",
    "    exit()\n",
    "\n",
    "# Normalizar las entradas para evitar errores de división por cero\n",
    "X_max = np.max(X, axis=0)\n",
    "if np.any(X_max == 0):\n",
    "    print(\"Error: Algunos valores máximos de las columnas de entrada son 0, lo que causará división por cero.\")\n",
    "    exit()\n",
    "X = X / X_max  # Normalizar de 0 a 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Explicación \n",
    "\n",
    "Este bloque de código divide los datos disponibles en dos grupos: uno para entrenar el modelo y otro para probarlo. Esto asegura que el modelo pueda ser evaluado de manera justa con datos que no ha visto antes.\n",
    "\n",
    "\n",
    "#### 1. Dividir los Datos en Entrenamiento y Prueba\n",
    "```python\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Datos divididos en entrenamiento ({X_train.shape[0]}) y prueba ({X_test.shape[0]}).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al dividir los datos de entrenamiento y prueba: {e}\")\n",
    "    exit()\n",
    "```\n",
    "- **¿Qué significa esto?**\n",
    "  - Divide las entradas (`X`) y salidas (`y`) en dos partes:\n",
    "    - **Entrenamiento** (`X_train`, `y_train`): Datos utilizados para construir el modelo.\n",
    "    - **Prueba** (`X_test`, `y_test`): Datos utilizados para evaluar qué tan bien funciona el modelo.\n",
    "  - El tamaño de los datos de prueba se define con `test_size=0.2`, lo que significa que el 20% de los datos se usará para pruebas y el 80% para entrenamiento.\n",
    "\n",
    "#### 2. Parámetros Importantes\n",
    "- **`test_size=0.2`**:\n",
    "  - Proporción de datos asignados al conjunto de prueba.\n",
    "  - En este caso, el 20% de los datos se destina a prueba y el 80% a entrenamiento.\n",
    "- **`random_state=42`**:\n",
    "  - Asegura que la división de los datos sea reproducible, es decir, siempre se obtendrán los mismos conjuntos de entrenamiento y prueba.\n",
    "- **`X_train.shape[0]` y `X_test.shape[0]`**:\n",
    "  - Devuelven el número de filas (ejemplos) en los conjuntos de entrenamiento y prueba respectivamente.\n",
    "\n",
    "#### 3. Manejar Errores\n",
    "```python\n",
    "except Exception as e:\n",
    "    print(f\"Error al dividir los datos de entrenamiento y prueba: {e}\")\n",
    "    exit()\n",
    "```\n",
    "- **¿Qué hace esto?**\n",
    "  - Si ocurre algún problema durante la división de los datos (por ejemplo, si las entradas y salidas no tienen el mismo tamaño), muestra un mensaje de error y detiene el programa.\n",
    "\n",
    "### Propósito General\n",
    "Este código asegura que:\n",
    "1. Los datos se dividan correctamente para construir y evaluar el modelo.\n",
    "2. Se mantenga una proporción fija entre los datos de entrenamiento y prueba.\n",
    "3. Se manejen errores potenciales que podrían surgir debido a problemas con los datos.\n",
    "\n",
    "### Ejemplo de Salida\n",
    "1. **División Exitosa**:\n",
    "   ```\n",
    "   Datos divididos en entrenamiento (80) y prueba (20).\n",
    "   ```\n",
    "2. **Error Durante la División**:\n",
    "   ```\n",
    "   Error al dividir los datos de entrenamiento y prueba: Las dimensiones de X e y no coinciden.\n",
    "   ```\n",
    "\n",
    "Esto facilita tanto el entrenamiento como la evaluación del modelo al garantizar que las etapas estén separadas y que el conjunto de prueba no interfiera con el proceso de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos divididos en entrenamiento (2989) y prueba (748).\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Datos divididos en entrenamiento ({X_train.shape[0]}) y prueba ({X_test.shape[0]}).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al dividir los datos de entrenamiento y prueba: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación \n",
    "\n",
    "Este bloque de código crea un modelo de red neuronal utilizando la biblioteca TensorFlow y Keras. Este modelo está diseñado para procesar datos con un número específico de entradas y generar cuatro resultados como salida.\n",
    "\n",
    "### Código Explicado\n",
    "\n",
    "#### 1. Crear el Modelo\n",
    "```python\n",
    "try:\n",
    "    input_shape = (X_train.shape[1],)  # Determinar automáticamente el número de características de entrada\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),        # Entrada con la forma del dataset\n",
    "        tf.keras.layers.Dense(8, activation='relu'),  # Capa oculta con 8 neuronas\n",
    "        tf.keras.layers.Dense(4, activation='sigmoid') # Cuatro salidas (para IN1, IN2, IN3, IN4)\n",
    "    ])\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear el modelo de la red neuronal: {e}\")\n",
    "    exit()\n",
    "```\n",
    "- **¿Qué significa esto?**\n",
    "  - Se crea una red neuronal con tres capas:\n",
    "    1. **Capa de entrada**: Toma datos con una cantidad específica de características (determinada automáticamente por `X_train.shape[1]`).\n",
    "    2. **Capa oculta**: Tiene 8 \"neuronas\" y utiliza la función de activación **ReLU** (Rectified Linear Unit), que ayuda a capturar relaciones complejas en los datos.\n",
    "    3. **Capa de salida**: Tiene 4 neuronas y utiliza la función de activación **sigmoid**, ideal para generar resultados en el rango [0, 1], típicamente usados para tareas de clasificación, para este caso de uso se utiliza por que las señales de activacion de los motores es digital.\n",
    "\n",
    "#### 2. Parámetros Importantes\n",
    "- **`input_shape`**:\n",
    "  - Define la cantidad de características de entrada que el modelo debe esperar (por ejemplo, 4 si hay 4 columnas en `X_train`).\n",
    "- **`Dense(8, activation='relu')`**:\n",
    "  - Una capa completamente conectada con 8 neuronas.\n",
    "  - **ReLU** ayuda al modelo a aprender patrones no lineales.\n",
    "- **`Dense(4, activation='sigmoid')`**:\n",
    "  - Una capa completamente conectada con 4 neuronas (una para cada salida esperada, como IN1, IN2, IN3, IN4).\n",
    "  - **Sigmoid** asegura que las salidas estén en el rango de 0 a 1.\n",
    "\n",
    "#### 3. Manejar Errores\n",
    "```python\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear el modelo de la red neuronal: {e}\")\n",
    "    exit()\n",
    "```\n",
    "- **¿Qué hace esto?**\n",
    "  - Si ocurre algún problema durante la creación del modelo (por ejemplo, si los datos de entrada no están bien definidos), muestra un mensaje de error y detiene el programa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo de la red neuronal\n",
    "try:\n",
    "    input_shape = (X_train.shape[1],)  # Determinar automáticamente el número de características de entrada\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),        # Entrada con la forma del dataset\n",
    "        tf.keras.layers.Dense(8, activation='relu'),  # Capa oculta con 8 neuronas\n",
    "        tf.keras.layers.Dense(4, activation='sigmoid') # Cuatro salidas (para IN1, IN2, IN3, IN4)\n",
    "    ])\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear el modelo de la red neuronal: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación\n",
    "\n",
    "Este bloque de código realiza dos pasos esenciales en el desarrollo de una red neuronal: compilar el modelo y entrenarlo con los datos.\n",
    "\n",
    "### Código Explicado\n",
    "\n",
    "#### 1. Compilar el Modelo\n",
    "```python\n",
    "try:\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "except Exception as e:\n",
    "    print(f\"Error al compilar el modelo: {e}\")\n",
    "    exit()\n",
    "```\n",
    "- **¿Qué significa esto?**\n",
    "  - **Compilar el modelo** prepara la red neuronal para ser entrenada, definiendo:\n",
    "    1. **Optimizador (`optimizer='adam'`)**: Controla cómo el modelo ajusta sus parámetros para mejorar el rendimiento.\n",
    "    2. **Función de pérdida (`loss='binary_crossentropy'`)**: Calcula qué tan lejos están las predicciones del modelo de los valores reales. Es adecuada para problemas con salidas binarias (0 o 1).\n",
    "    3. **Métrica (`metrics=['accuracy']`)**: Supervisa la precisión durante el entrenamiento para evaluar el desempeño del modelo.\n",
    "\n",
    "- **Manejo de Errores**:\n",
    "  - Si algo falla al compilar (por ejemplo, configuraciones incompatibles), muestra un mensaje de error y detiene el programa.\n",
    "\n",
    "#### 2. Entrenar el Modelo\n",
    "```python\n",
    "try:\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.2, verbose=1)\n",
    "    print(\"Entrenamiento completado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el entrenamiento del modelo: {e}\")\n",
    "    exit()\n",
    "```\n",
    "- **¿Qué significa esto?**\n",
    "  - **Entrenar el modelo** significa enseñarle a predecir correctamente usando los datos de entrenamiento.\n",
    "  - Los parámetros importantes son:\n",
    "    1. **`epochs=50`**: El número de veces que el modelo verá todos los datos durante el entrenamiento. Más épocas pueden mejorar el aprendizaje, pero también aumentar el riesgo de \"sobreajuste\".\n",
    "    2. **`batch_size=8`**: El modelo procesa los datos en lotes de 8 elementos para mejorar la eficiencia.\n",
    "    3. **`validation_split=0.2`**: Reserva el 20% de los datos de entrenamiento para validación, ayudando a medir el rendimiento del modelo en datos no vistos.\n",
    "    4. **`verbose=1`**: Muestra el progreso del entrenamiento en la consola.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1125 - loss: 0.7021 - val_accuracy: 0.1789 - val_loss: 0.6605\n",
      "Epoch 2/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2201 - loss: 0.6561 - val_accuracy: 0.2174 - val_loss: 0.6474\n",
      "Epoch 3/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2199 - loss: 0.6450 - val_accuracy: 0.2358 - val_loss: 0.6415\n",
      "Epoch 4/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2327 - loss: 0.6428 - val_accuracy: 0.2241 - val_loss: 0.6359\n",
      "Epoch 5/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2167 - loss: 0.6323 - val_accuracy: 0.2408 - val_loss: 0.6301\n",
      "Epoch 6/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2235 - loss: 0.6233 - val_accuracy: 0.2542 - val_loss: 0.6237\n",
      "Epoch 7/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2434 - loss: 0.6227 - val_accuracy: 0.2324 - val_loss: 0.6172\n",
      "Epoch 8/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2384 - loss: 0.6152 - val_accuracy: 0.2375 - val_loss: 0.6115\n",
      "Epoch 9/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2252 - loss: 0.6085 - val_accuracy: 0.2742 - val_loss: 0.6067\n",
      "Epoch 10/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2703 - loss: 0.6052 - val_accuracy: 0.2742 - val_loss: 0.6035\n",
      "Epoch 11/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3101 - loss: 0.6013 - val_accuracy: 0.3144 - val_loss: 0.6010\n",
      "Epoch 12/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3759 - loss: 0.6020 - val_accuracy: 0.3144 - val_loss: 0.5996\n",
      "Epoch 13/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3068 - loss: 0.5988 - val_accuracy: 0.3896 - val_loss: 0.5985\n",
      "Epoch 14/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3930 - loss: 0.5881 - val_accuracy: 0.3930 - val_loss: 0.5976\n",
      "Epoch 15/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3846 - loss: 0.5944 - val_accuracy: 0.3763 - val_loss: 0.5967\n",
      "Epoch 16/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3911 - loss: 0.5992 - val_accuracy: 0.3696 - val_loss: 0.5965\n",
      "Epoch 17/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3601 - loss: 0.6035 - val_accuracy: 0.3913 - val_loss: 0.5957\n",
      "Epoch 18/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3904 - loss: 0.5993 - val_accuracy: 0.3796 - val_loss: 0.5953\n",
      "Epoch 19/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3898 - loss: 0.5961 - val_accuracy: 0.3813 - val_loss: 0.5948\n",
      "Epoch 20/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3939 - loss: 0.5910 - val_accuracy: 0.3779 - val_loss: 0.5943\n",
      "Epoch 21/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3796 - loss: 0.5894 - val_accuracy: 0.3779 - val_loss: 0.5940\n",
      "Epoch 22/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3879 - loss: 0.5947 - val_accuracy: 0.3813 - val_loss: 0.5935\n",
      "Epoch 23/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3676 - loss: 0.5936 - val_accuracy: 0.3863 - val_loss: 0.5929\n",
      "Epoch 24/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4196 - loss: 0.5825 - val_accuracy: 0.3712 - val_loss: 0.5928\n",
      "Epoch 25/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3672 - loss: 0.5933 - val_accuracy: 0.3779 - val_loss: 0.5924\n",
      "Epoch 26/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3884 - loss: 0.5957 - val_accuracy: 0.3779 - val_loss: 0.5919\n",
      "Epoch 27/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3787 - loss: 0.6060 - val_accuracy: 0.3946 - val_loss: 0.5910\n",
      "Epoch 28/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4145 - loss: 0.5854 - val_accuracy: 0.3880 - val_loss: 0.5904\n",
      "Epoch 29/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4062 - loss: 0.5925 - val_accuracy: 0.3746 - val_loss: 0.5903\n",
      "Epoch 30/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4025 - loss: 0.5954 - val_accuracy: 0.3829 - val_loss: 0.5893\n",
      "Epoch 31/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4067 - loss: 0.5839 - val_accuracy: 0.3846 - val_loss: 0.5888\n",
      "Epoch 32/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4407 - loss: 0.5799 - val_accuracy: 0.3863 - val_loss: 0.5881\n",
      "Epoch 33/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4103 - loss: 0.5855 - val_accuracy: 0.4064 - val_loss: 0.5877\n",
      "Epoch 34/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4047 - loss: 0.5845 - val_accuracy: 0.3963 - val_loss: 0.5875\n",
      "Epoch 35/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4384 - loss: 0.5804 - val_accuracy: 0.4147 - val_loss: 0.5866\n",
      "Epoch 36/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4426 - loss: 0.5837 - val_accuracy: 0.4013 - val_loss: 0.5857\n",
      "Epoch 37/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4232 - loss: 0.5725 - val_accuracy: 0.3997 - val_loss: 0.5853\n",
      "Epoch 38/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4200 - loss: 0.5879 - val_accuracy: 0.4181 - val_loss: 0.5848\n",
      "Epoch 39/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4283 - loss: 0.5805 - val_accuracy: 0.4114 - val_loss: 0.5836\n",
      "Epoch 40/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4399 - loss: 0.5835 - val_accuracy: 0.4013 - val_loss: 0.5830\n",
      "Epoch 41/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4213 - loss: 0.5744 - val_accuracy: 0.4047 - val_loss: 0.5822\n",
      "Epoch 42/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4383 - loss: 0.5723 - val_accuracy: 0.4064 - val_loss: 0.5825\n",
      "Epoch 43/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4388 - loss: 0.5686 - val_accuracy: 0.4130 - val_loss: 0.5822\n",
      "Epoch 44/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4240 - loss: 0.5724 - val_accuracy: 0.4130 - val_loss: 0.5805\n",
      "Epoch 45/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4335 - loss: 0.5828 - val_accuracy: 0.4080 - val_loss: 0.5798\n",
      "Epoch 46/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4340 - loss: 0.5660 - val_accuracy: 0.4164 - val_loss: 0.5791\n",
      "Epoch 47/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4230 - loss: 0.5792 - val_accuracy: 0.4114 - val_loss: 0.5787\n",
      "Epoch 48/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4245 - loss: 0.5750 - val_accuracy: 0.4181 - val_loss: 0.5776\n",
      "Epoch 49/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4394 - loss: 0.5779 - val_accuracy: 0.4214 - val_loss: 0.5785\n",
      "Epoch 50/50\n",
      "\u001b[1m299/299\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4583 - loss: 0.5681 - val_accuracy: 0.4064 - val_loss: 0.5768\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# Compilar el modelo\n",
    "try:\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "except Exception as e:\n",
    "    print(f\"Error al compilar el modelo: {e}\")\n",
    "    exit()\n",
    "# Entrenar el modelo\n",
    "try:\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.2, verbose=1)\n",
    "    print(\"Entrenamiento completado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el entrenamiento del modelo: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación\n",
    "\n",
    "Este bloque de código evalúa el desempeño del modelo de red neuronal utilizando los datos de prueba. Esto ayuda a determinar qué tan bien funciona el modelo con datos que no ha visto antes.\n",
    "\n",
    "\n",
    "#### Evaluar el Modelo\n",
    "```python\n",
    "try:\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f'Pérdida de prueba: {test_loss:.4f}, Precisión de prueba: {test_accuracy:.4f}')\n",
    "except Exception as e:\n",
    "    print(f\"Error al evaluar el modelo: {e}\")\n",
    "    exit()\n",
    "```\n",
    "- **¿Qué significa esto?**\n",
    "  - La función `model.evaluate()` calcula:\n",
    "    1. **Pérdida de prueba (`test_loss`)**: Una medida que indica qué tan lejos están las predicciones del modelo de los valores reales. Un valor más bajo es mejor.\n",
    "    2. **Precisión de prueba (`test_accuracy`)**: Indica el porcentaje de predicciones correctas hechas por el modelo en el conjunto de prueba. Un valor más alto es mejor.\n",
    "\n",
    "- **Parámetros Importantes**:\n",
    "  - **`X_test` y `y_test`**: Los datos de prueba que el modelo no ha visto durante el entrenamiento, usados para evaluar su desempeño.\n",
    "  - **`verbose=2`**: Muestra un nivel intermedio de detalles sobre el proceso de evaluación.\n",
    "\n",
    "- **Manejo de Errores**:\n",
    "  - Si algo falla durante la evaluación (por ejemplo, datos mal estructurados o un modelo mal configurado), se captura la excepción, se imprime un mensaje de error y el programa se detiene.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 0s - 2ms/step - accuracy: 0.4332 - loss: 0.5771\n",
      "Pérdida de prueba: 0.5771, Precisión de prueba: 0.4332\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo con los datos de prueba\n",
    "try:\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f'Pérdida de prueba: {test_loss:.4f}, Precisión de prueba: {test_accuracy:.4f}')\n",
    "except Exception as e:\n",
    "    print(f\"Error al evaluar el modelo: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación \n",
    "\n",
    "Este bloque de código genera un archivo de cabecera (`.h`) que contiene los pesos y sesgos del modelo de red neuronal en formato C/C++. Este archivo es útil para integrar los parámetros del modelo en aplicaciones de hardware o sistemas embebidos.\n",
    "\n",
    "#### 1. Definir la Ruta del Archivo de Salida\n",
    "```python\n",
    "carpeta_de_salida = Carpeta_de_entrenamiento.replace('datos_iniciales_del_sistema', 'salida_archivo_con_pesos_red_neuronal')\n",
    "ruta_h = os.path.join(carpeta_de_salida, 'pesos_red_neuronal.h')\n",
    "```\n",
    "- **¿Qué significa esto?**\n",
    "  - Se define una carpeta para almacenar el archivo generado, reemplazando el directorio base `datos_iniciales_del_sistema` por `salida_archivo_con_pesos_red_neuronal`.\n",
    "  - Se construye la ruta completa para el archivo de salida llamado `pesos_red_neuronal.h`.\n",
    "\n",
    "\n",
    "- **¿Qué hace esto?**\n",
    "  1. Abre (o crea) un archivo `.h` en la ubicación especificada.\n",
    "  2. Escribe las declaraciones necesarias para definir constantes de pesos y sesgos para cada capa del modelo.\n",
    "  3. Cada capa contiene:\n",
    "     - **Pesos**: Representados como un arreglo unidimensional en C.\n",
    "     - **Sesgos**: También representados como un arreglo en C.\n",
    "  4. Utiliza bucles para recorrer todas las capas del modelo y extraer los valores.\n",
    "  5. Maneja errores que puedan ocurrir al obtener los pesos y sesgos de una capa específica.\n",
    "\n",
    "- **Estructura del Archivo Generado**:\n",
    "  - Comienza con definiciones para evitar múltiples inclusiones (`#ifndef ... #define ...`).\n",
    "  - Contiene los pesos y sesgos para cada capa en formato de arreglo estático de punto flotante.\n",
    "  - Finaliza con una directiva para cerrar el bloque de inclusión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de pesos generado exitosamente en: c:\\Users\\dgome\\OneDrive\\SIN RECUPERAR\\Sql\\Documentos\\SistemaIot\\Periferico\\modelos_entrenamiento\\vehiculo_autonomo\\salida_archivo_con_pesos_red_neuronal\\pesos_red_neuronal.h\n"
     ]
    }
   ],
   "source": [
    "# Generar el archivo de cabecera (.h) para los pesos y sesgos\n",
    "carpeta_de_salida=Carpeta_de_entrenamiento.replace('datos_iniciales_del_sistema','salida_archivo_con_pesos_red_neuronal')\n",
    "\n",
    "\n",
    "ruta_h = os.path.join(carpeta_de_salida, 'pesos_red_neuronal.h')\n",
    "\n",
    "try:\n",
    "    with open(ruta_h, 'w') as archivo_h:\n",
    "        archivo_h.write(\"#ifndef PESOS_RED_NEURONAL_H\\n\")\n",
    "        archivo_h.write(\"#define PESOS_RED_NEURONAL_H\\n\\n\")\n",
    "        \n",
    "        for i, layer in enumerate(model.layers):\n",
    "            try:\n",
    "                weights, biases = layer.get_weights()\n",
    "                \n",
    "                archivo_h.write(f\"// Pesos de la capa {i}\\n\")\n",
    "                archivo_h.write(f\"static const float PESOS_CAPA_{i}[] = {{\\n\")\n",
    "                \n",
    "                flat_weights = weights.flatten()\n",
    "                archivo_h.write(\", \".join([f\"{w:.6f}\" for w in flat_weights]))\n",
    "                archivo_h.write(\"\\n};\\n\\n\")\n",
    "                \n",
    "                archivo_h.write(f\"// Sesgos de la capa {i}\\n\")\n",
    "                archivo_h.write(f\"static const float SESGOS_CAPA_{i}[] = {{\\n\")\n",
    "                archivo_h.write(\", \".join([f\"{b:.6f}\" for b in biases]))\n",
    "                archivo_h.write(\"\\n};\\n\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al obtener los pesos y sesgos de la capa {i}: {e}\")\n",
    "        \n",
    "        archivo_h.write(\"#endif // PESOS_RED_NEURONAL_H\\n\")\n",
    "        print(f\"Archivo de pesos generado exitosamente en: {ruta_h}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al escribir el archivo de pesos: {e}\")\n",
    "    exit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
